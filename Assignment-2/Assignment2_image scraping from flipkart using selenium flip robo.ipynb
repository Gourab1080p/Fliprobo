{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import shutil\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directory(dirname):\n",
    "    current_path=os.getcwd()\n",
    "    path = os.path.join(current_path,dirname)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(data,dirname,page):\n",
    "    for index,link in enumerate(data['image_urls']):\n",
    "        print(\"Downloading {0} of {1} images\".format(index+1,len(data['image_urls'])))\n",
    "        response = requests.get(link)\n",
    "        with open ('{0}/img_{1}{2}.jpeg'.format(dirname,page,index),\"wb\") as file:\n",
    "            file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_csv(data,filename):\n",
    "    df=pd.DataFrame(data)\n",
    "    df.to_csv(filename,mode=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sraping Images URLS Selenium\n",
    "def scrap_images_url(driver):\n",
    "    images = driver.find_elements_by_xpath(\"//img[@class='_3togXc']\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    product_data = {}\n",
    "    product_data['image_urls'] = []\n",
    "    \n",
    "    \n",
    "    for image in images:\n",
    "        source = image.get_attribute('src')\n",
    "        product_data['image_urls'].append(source)\n",
    "        print(\"Returing scraped data\")\n",
    "    \n",
    "    \n",
    "    return product_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver as wb\n",
    "from selenium.common.exceptions import StaleElementReferenceException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVER_PATH =r\"C:/Users/acer/Desktop/chromedriver.exe\"\n",
    "driver =wb.Chrome(executable_path=DRIVER_PATH)\n",
    "current_page_url = driver.get(\"https://www.flipkart.com/mens-clothing/trousers/pr?sid=2oq,s9b,9uj&otracker=nmenu_sub_Men_0_Casual%20Trousers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRNAME=\"Trouser\"\n",
    "\n",
    "\n",
    "make_directory(DIRNAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Scraping page (0) fo (1)page\n",
      "The current page scaped is\n",
      "Downloading 1 of 40 images\n",
      "Downloading 2 of 40 images\n",
      "Downloading 3 of 40 images\n",
      "Downloading 4 of 40 images\n",
      "Downloading 5 of 40 images\n",
      "Downloading 6 of 40 images\n",
      "Downloading 7 of 40 images\n",
      "Downloading 8 of 40 images\n",
      "Downloading 9 of 40 images\n",
      "Downloading 10 of 40 images\n",
      "Downloading 11 of 40 images\n",
      "Downloading 12 of 40 images\n",
      "Downloading 13 of 40 images\n",
      "Downloading 14 of 40 images\n",
      "Downloading 15 of 40 images\n",
      "Downloading 16 of 40 images\n",
      "Downloading 17 of 40 images\n",
      "Downloading 18 of 40 images\n",
      "Downloading 19 of 40 images\n",
      "Downloading 20 of 40 images\n",
      "Downloading 21 of 40 images\n",
      "Downloading 22 of 40 images\n",
      "Downloading 23 of 40 images\n",
      "Downloading 24 of 40 images\n",
      "Downloading 25 of 40 images\n",
      "Downloading 26 of 40 images\n",
      "Downloading 27 of 40 images\n",
      "Downloading 28 of 40 images\n",
      "Downloading 29 of 40 images\n",
      "Downloading 30 of 40 images\n",
      "Downloading 31 of 40 images\n",
      "Downloading 32 of 40 images\n",
      "Downloading 33 of 40 images\n",
      "Downloading 34 of 40 images\n",
      "Downloading 35 of 40 images\n",
      "Downloading 36 of 40 images\n",
      "Downloading 37 of 40 images\n",
      "Downloading 38 of 40 images\n",
      "Downloading 39 of 40 images\n",
      "Downloading 40 of 40 images\n",
      "Scraping of page (0) done\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Scraping page (0) fo (1)page\n",
      "The current page scaped is\n",
      "Downloading 1 of 40 images\n",
      "Downloading 2 of 40 images\n",
      "Downloading 3 of 40 images\n",
      "Downloading 4 of 40 images\n",
      "Downloading 5 of 40 images\n",
      "Downloading 6 of 40 images\n",
      "Downloading 7 of 40 images\n",
      "Downloading 8 of 40 images\n",
      "Downloading 9 of 40 images\n",
      "Downloading 10 of 40 images\n",
      "Downloading 11 of 40 images\n",
      "Downloading 12 of 40 images\n",
      "Downloading 13 of 40 images\n",
      "Downloading 14 of 40 images\n",
      "Downloading 15 of 40 images\n",
      "Downloading 16 of 40 images\n",
      "Downloading 17 of 40 images\n",
      "Downloading 18 of 40 images\n",
      "Downloading 19 of 40 images\n",
      "Downloading 20 of 40 images\n",
      "Downloading 21 of 40 images\n",
      "Downloading 22 of 40 images\n",
      "Downloading 23 of 40 images\n",
      "Downloading 24 of 40 images\n",
      "Downloading 25 of 40 images\n",
      "Downloading 26 of 40 images\n",
      "Downloading 27 of 40 images\n",
      "Downloading 28 of 40 images\n",
      "Downloading 29 of 40 images\n",
      "Downloading 30 of 40 images\n",
      "Downloading 31 of 40 images\n",
      "Downloading 32 of 40 images\n",
      "Downloading 33 of 40 images\n",
      "Downloading 34 of 40 images\n",
      "Downloading 35 of 40 images\n",
      "Downloading 36 of 40 images\n",
      "Downloading 37 of 40 images\n",
      "Downloading 38 of 40 images\n",
      "Downloading 39 of 40 images\n",
      "Downloading 40 of 40 images\n",
      "Scraping of page (0) done\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Scraping page (0) fo (1)page\n",
      "The current page scaped is\n",
      "Downloading 1 of 40 images\n",
      "Downloading 2 of 40 images\n",
      "Downloading 3 of 40 images\n",
      "Downloading 4 of 40 images\n",
      "Downloading 5 of 40 images\n",
      "Downloading 6 of 40 images\n",
      "Downloading 7 of 40 images\n",
      "Downloading 8 of 40 images\n",
      "Downloading 9 of 40 images\n",
      "Downloading 10 of 40 images\n",
      "Downloading 11 of 40 images\n",
      "Downloading 12 of 40 images\n",
      "Downloading 13 of 40 images\n",
      "Downloading 14 of 40 images\n",
      "Downloading 15 of 40 images\n",
      "Downloading 16 of 40 images\n",
      "Downloading 17 of 40 images\n",
      "Downloading 18 of 40 images\n",
      "Downloading 19 of 40 images\n",
      "Downloading 20 of 40 images\n",
      "Downloading 21 of 40 images\n",
      "Downloading 22 of 40 images\n",
      "Downloading 23 of 40 images\n",
      "Downloading 24 of 40 images\n",
      "Downloading 25 of 40 images\n",
      "Downloading 26 of 40 images\n",
      "Downloading 27 of 40 images\n",
      "Downloading 28 of 40 images\n",
      "Downloading 29 of 40 images\n",
      "Downloading 30 of 40 images\n",
      "Downloading 31 of 40 images\n",
      "Downloading 32 of 40 images\n",
      "Downloading 33 of 40 images\n",
      "Downloading 34 of 40 images\n",
      "Downloading 35 of 40 images\n",
      "Downloading 36 of 40 images\n",
      "Downloading 37 of 40 images\n",
      "Downloading 38 of 40 images\n",
      "Downloading 39 of 40 images\n",
      "Downloading 40 of 40 images\n",
      "Scraping of page (0) done\n"
     ]
    }
   ],
   "source": [
    "start_page=1\n",
    "total_pages=3\n",
    "#scraping the image\n",
    "for page in range(start_page,total_pages+1):\n",
    "    try:\n",
    "        product_details=scrap_images_url(driver=driver)\n",
    "        print(\"Scraping page (0) fo (1)page\".format(page,total_pages))\n",
    "        page_value=driver.find_element_by_xpath(\"//img[@class='_3togXc']\").text\n",
    "        print(\"The current page scaped is{}\".format(page_value))\n",
    "        ##Downloading the image\n",
    "        save_images(data=product_details,dirname=DIRNAME,page=page)\n",
    "        print(\"Scraping of page (0) done\".format(page))\n",
    "        #saving the data into a csv file_x\n",
    "        save_data_to_csv(data=product_details,filename=\"Trouser.csv\")\n",
    "       \n",
    "      \n",
    "        \n",
    "    except StaleElementReferenceException as Exception:\n",
    "        print(\"We are facing a exception\")\n",
    "        exp_page=driver.find_element_by_path(\"//a[@class='_3togXc']\").text\n",
    "        print(\"The Page value at the time of exception is {}\".format(exp_page))\n",
    "        \n",
    "        value=driver.find_element_by_path(\"//a[@class='_3ZJShS31bMyl']\")\n",
    "        link=value.get_attribute('href')\n",
    "        driver.get(link)\n",
    "        \n",
    "        product_details=scrap_image_url(driver=driver)\n",
    "        print(\"Scraping page (0) fo (1)pages\".format(page,total_pages))\n",
    "        \n",
    "        page_value=driver.find_element_by_xpath(\"//a[@class='_3ZJShS31bMyl']\").text\n",
    "        print(\"The current page scaped is{}\".format(page_value))\n",
    "    \n",
    "        ##Downloading the image\n",
    "        save_images(data=product_details,dirname=DIRNAME,page=page)\n",
    "        print(\"Scraping of page (0) done\".format(page))\n",
    "        #saving the data into a csv file_x\n",
    "        save_data_to_csv(data=product_details,filename=\"Trouser.csv\")\n",
    "      \n",
    "        print('new_page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVER_PATH =r\"C:/Users/acer/Desktop/chromedriver.exe\"\n",
    "driver =wb.Chrome(executable_path=DRIVER_PATH)\n",
    "page2 =driver.get(\"https://www.flipkart.com/clothing-and-accessories/bottomwear/jeans/men-jeans/pr?sid=clo,vua,k58,i51&otracker=categorytree&otracker=nmenu_sub_Men_0_Jeans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directory1(dirname):\n",
    "    current_path=os.getcwd()\n",
    "    path = os.path.join(current_path,dirname)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(data,dirname,page):\n",
    "    for index,link in enumerate(data['image_urls']):\n",
    "        print(\"Downloading {0} of {1} images\".format(index+1,len(data['image_urls'])))\n",
    "        response = requests.get(link)\n",
    "        with open ('{0}/img_{1}{2}.jpeg'.format(dirname,page,index),\"wb\") as file:\n",
    "            file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_csv(data,filename):\n",
    "    df=pd.DataFrame(data)\n",
    "    df.to_csv(filename,mode=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRNAME1=\"Jeans\"\n",
    "make_directory1(DIRNAME1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Scraping page (0) fo (1)page\n",
      "The current page scaped is\n",
      "Downloading 1 of 40 images\n",
      "Downloading 2 of 40 images\n",
      "Downloading 3 of 40 images\n",
      "Downloading 4 of 40 images\n",
      "Downloading 5 of 40 images\n",
      "Downloading 6 of 40 images\n",
      "Downloading 7 of 40 images\n",
      "Downloading 8 of 40 images\n",
      "Downloading 9 of 40 images\n",
      "Downloading 10 of 40 images\n",
      "Downloading 11 of 40 images\n",
      "Downloading 12 of 40 images\n",
      "Downloading 13 of 40 images\n",
      "Downloading 14 of 40 images\n",
      "Downloading 15 of 40 images\n",
      "Downloading 16 of 40 images\n",
      "Downloading 17 of 40 images\n",
      "Downloading 18 of 40 images\n",
      "Downloading 19 of 40 images\n",
      "Downloading 20 of 40 images\n",
      "Downloading 21 of 40 images\n",
      "Downloading 22 of 40 images\n",
      "Downloading 23 of 40 images\n",
      "Downloading 24 of 40 images\n",
      "Downloading 25 of 40 images\n",
      "Downloading 26 of 40 images\n",
      "Downloading 27 of 40 images\n",
      "Downloading 28 of 40 images\n",
      "Downloading 29 of 40 images\n",
      "Downloading 30 of 40 images\n",
      "Downloading 31 of 40 images\n",
      "Downloading 32 of 40 images\n",
      "Downloading 33 of 40 images\n",
      "Downloading 34 of 40 images\n",
      "Downloading 35 of 40 images\n",
      "Downloading 36 of 40 images\n",
      "Downloading 37 of 40 images\n",
      "Downloading 38 of 40 images\n",
      "Downloading 39 of 40 images\n",
      "Downloading 40 of 40 images\n",
      "Scraping of page (0) done\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Scraping page (0) fo (1)page\n",
      "The current page scaped is\n",
      "Downloading 1 of 40 images\n",
      "Downloading 2 of 40 images\n",
      "Downloading 3 of 40 images\n",
      "Downloading 4 of 40 images\n",
      "Downloading 5 of 40 images\n",
      "Downloading 6 of 40 images\n",
      "Downloading 7 of 40 images\n",
      "Downloading 8 of 40 images\n",
      "Downloading 9 of 40 images\n",
      "Downloading 10 of 40 images\n",
      "Downloading 11 of 40 images\n",
      "Downloading 12 of 40 images\n",
      "Downloading 13 of 40 images\n",
      "Downloading 14 of 40 images\n",
      "Downloading 15 of 40 images\n",
      "Downloading 16 of 40 images\n",
      "Downloading 17 of 40 images\n",
      "Downloading 18 of 40 images\n",
      "Downloading 19 of 40 images\n",
      "Downloading 20 of 40 images\n",
      "Downloading 21 of 40 images\n",
      "Downloading 22 of 40 images\n",
      "Downloading 23 of 40 images\n",
      "Downloading 24 of 40 images\n",
      "Downloading 25 of 40 images\n",
      "Downloading 26 of 40 images\n",
      "Downloading 27 of 40 images\n",
      "Downloading 28 of 40 images\n",
      "Downloading 29 of 40 images\n",
      "Downloading 30 of 40 images\n",
      "Downloading 31 of 40 images\n",
      "Downloading 32 of 40 images\n",
      "Downloading 33 of 40 images\n",
      "Downloading 34 of 40 images\n",
      "Downloading 35 of 40 images\n",
      "Downloading 36 of 40 images\n",
      "Downloading 37 of 40 images\n",
      "Downloading 38 of 40 images\n",
      "Downloading 39 of 40 images\n",
      "Downloading 40 of 40 images\n",
      "Scraping of page (0) done\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Scraping page (0) fo (1)page\n",
      "The current page scaped is\n",
      "Downloading 1 of 40 images\n",
      "Downloading 2 of 40 images\n",
      "Downloading 3 of 40 images\n",
      "Downloading 4 of 40 images\n",
      "Downloading 5 of 40 images\n",
      "Downloading 6 of 40 images\n",
      "Downloading 7 of 40 images\n",
      "Downloading 8 of 40 images\n",
      "Downloading 9 of 40 images\n",
      "Downloading 10 of 40 images\n",
      "Downloading 11 of 40 images\n",
      "Downloading 12 of 40 images\n",
      "Downloading 13 of 40 images\n",
      "Downloading 14 of 40 images\n",
      "Downloading 15 of 40 images\n",
      "Downloading 16 of 40 images\n",
      "Downloading 17 of 40 images\n",
      "Downloading 18 of 40 images\n",
      "Downloading 19 of 40 images\n",
      "Downloading 20 of 40 images\n",
      "Downloading 21 of 40 images\n",
      "Downloading 22 of 40 images\n",
      "Downloading 23 of 40 images\n",
      "Downloading 24 of 40 images\n",
      "Downloading 25 of 40 images\n",
      "Downloading 26 of 40 images\n",
      "Downloading 27 of 40 images\n",
      "Downloading 28 of 40 images\n",
      "Downloading 29 of 40 images\n",
      "Downloading 30 of 40 images\n",
      "Downloading 31 of 40 images\n",
      "Downloading 32 of 40 images\n",
      "Downloading 33 of 40 images\n",
      "Downloading 34 of 40 images\n",
      "Downloading 35 of 40 images\n",
      "Downloading 36 of 40 images\n",
      "Downloading 37 of 40 images\n",
      "Downloading 38 of 40 images\n",
      "Downloading 39 of 40 images\n",
      "Downloading 40 of 40 images\n",
      "Scraping of page (0) done\n"
     ]
    }
   ],
   "source": [
    "start_page=1\n",
    "total_pages=3\n",
    "#scraping the image\n",
    "for page in range(start_page,total_pages+1):\n",
    "    try:\n",
    "        product_details=scrap_images_url(driver=driver)\n",
    "        print(\"Scraping page (0) fo (1)page\".format(page,total_pages))\n",
    "        page_value=driver.find_element_by_xpath(\"//img[@class='_3togXc']\").text\n",
    "        print(\"The current page scaped is{}\".format(page_value))\n",
    "        ##Downloading the image\n",
    "        save_images(data=product_details,dirname=DIRNAME1,page=page)\n",
    "        print(\"Scraping of page (0) done\".format(page))\n",
    "        #saving the data into a csv file_x\n",
    "        save_data_to_csv(data=product_details,filename=\"Jeans.csv\")\n",
    "       \n",
    "      \n",
    "        \n",
    "    except StaleElementReferenceException as Exception:\n",
    "        print(\"We are facing a exception\")\n",
    "        exp_page=driver.find_element_by_path(\"//a[@class='_3togXc']\").text\n",
    "        print(\"The Page value at the time of exception is {}\".format(exp_page))\n",
    "        \n",
    "        value=driver.find_element_by_path(\"//a[@class='_3ZJShS31bMyl']\")\n",
    "        link=value.get_attribute('href')\n",
    "        driver.get(link)\n",
    "        \n",
    "        product_details=scrap_image_url(driver=driver)\n",
    "        print(\"Scraping page (0) fo (1)pages\".format(page,total_pages))\n",
    "        \n",
    "        page_value=driver.find_element_by_xpath(\"//a[@class='_3ZJShS31bMyl']\").text\n",
    "        print(\"The current page scaped is{}\".format(page_value))\n",
    "    \n",
    "        ##Downloading the image\n",
    "        save_images(data=product_details,dirname=DIRNAME1,page=page)\n",
    "        print(\"Scraping of page (0) done\".format(page))\n",
    "        #saving the data into a csv file_x\n",
    "        save_data_to_csv(data=product_details,filename=\"Jeans.csv\")\n",
    "      \n",
    "        print('new_page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
